{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "from urllib import urlretrieve\n",
    "import os\n",
    "import random\n",
    "from os.path import isfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = '../SampleDatasets/ATIS/'\n",
    "\n",
    "\n",
    "def download_dropbox():\n",
    "    ''' \n",
    "    download from drop box in the meantime\n",
    "    '''\n",
    "    print('Downloading data from https://www.dropbox.com/s/3lxl9jsbw0j7h8a/atis.pkl?dl=0')\n",
    "    os.system('wget -O atis.pkl https://www.dropbox.com/s/3lxl9jsbw0j7h8a/atis.pkl?dl=0')\n",
    "    \n",
    "def load_dropbox(filename):\n",
    "    if not isfile(filename):\n",
    "        #download('http://www-etud.iro.umontreal.ca/~mesnilgr/atis/'+filename)\n",
    "        download_dropbox()\n",
    "    #f = gzip.open(filename,'rb')\n",
    "    f = open(filename,'rb')\n",
    "    return f\n",
    "\n",
    "def atisfull():\n",
    "    f = load_dropbox(PREFIX + 'atis.pkl')\n",
    "    \n",
    "    try:\n",
    "        train_set, test_set, dicts = pickle.load(f)\n",
    "    except UnicodeDecodeError:\n",
    "        train_set, test_set, dicts = pickle.load(f, encoding='latin1')\n",
    "    return train_set, test_set, dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, dicts = atisfull()\n",
    "w2idx, labels2idx = dicts['words2idx'], dicts['labels2idx']\n",
    "\n",
    "train_x, _, train_label = train_set\n",
    "val_x, _, val_label = valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index to word/label dicts\n",
    "idx2w  = {w2idx[k]:k for k in w2idx}\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conlleval script\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in train_x]\n",
    "labels_train = [ list(map(lambda x: idx2la[x], y)) for y in train_label]\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in val_x]\n",
    "labels_val = [ list(map(lambda x: idx2la[x], y)) for y in val_label]\n",
    "\n",
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence : ['i', 'want', 'to', 'fly', 'from', 'boston', 'at', 'DIGITDIGITDIGIT', 'am', 'and', 'arrive', 'in', 'denver', 'at', 'DIGITDIGITDIGITDIGIT', 'in', 'the', 'morning']\n",
      "Encoded form: [232 542 502 196 208  77  62  10  35  40  58 234 137  62  11 234 481 321]\n",
      "\n",
      "It's label : ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day']\n",
      "Encoded form: [126 126 126 126 126  48 126  35  99 126 126 126  78 126  14 126 126  12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence : {}\".format(words_train[0]))\n",
    "print(\"Encoded form: {}\".format(train_x[0]))\n",
    "print()\n",
    "print(\"It's label : {}\".format(labels_train[0]))\n",
    "print(\"Encoded form: {}\".format(train_label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(SimpleRNN(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(n_vocab,100))\n",
    "# model.add(Convolution1D(128, 5, border_mode='same', activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(GRU(100,return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "# model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "Training epoch 1\n",
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "Training epoch 21\n",
      "Training epoch 22\n",
      "Training epoch 23\n",
      "Training epoch 24\n",
      "Training epoch 25\n",
      "Training epoch 26\n",
      "Training epoch 27\n",
      "Training epoch 28\n",
      "Training epoch 29\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(\"Training epoch {}\".format(i))\n",
    "    \n",
    "    for n_batch, sent in enumerate(train_x):\n",
    "        label = train_label[n_batch]\n",
    "        # Make labels one hot\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:] \n",
    "        # View each sentence as a batch\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #ignore 1 word sentences\n",
    "            model.train_on_batch(sent, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import os\n",
    "# import stat\n",
    "import subprocess\n",
    "# from os.path import isfile, join\n",
    "# from os import chmod\n",
    "\n",
    "def conlleval(p, g, w, filename):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, w in zip(sl, sp, sw):\n",
    "            out += w + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "    \n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain\n",
    "    precision/recall and F1 score '''\n",
    "    _conlleval = 'conlleval.pl'\n",
    "#     if not isfile(_conlleval):\n",
    "#         #download('http://www-etud.iro.umontreal.ca/~mesnilgr/atis/conlleval.pl') \n",
    "#         os.system('wget https://www.comp.nus.edu.sg/%7Ekanmy/courses/practicalNLP_2008/packages/conlleval.pl')\n",
    "#         chmod('conlleval.pl', stat.S_IRWXU) # give the execute permissions\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename,'rb').read())\n",
    "    for line in stdout.decode(\"utf-8\").split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "    \n",
    "    # out = ['accuracy:', '16.26%;', 'precision:', '0.00%;', 'recall:', '0.00%;', 'FB1:', '0.00']\n",
    "    \n",
    "    precision = float(out[3][:-2])\n",
    "    recall    = float(out[5][:-2])\n",
    "    f1score   = float(out[7])\n",
    "\n",
    "    return {'p':precision, 'r':recall, 'f1':f1score}\n",
    "\n",
    "def get_perfo(filename):\n",
    "    ''' \n",
    "    work around for using a PERL script in python\n",
    "    dirty but still works.\n",
    "    '''\n",
    "    tempfile = str(random.randint(1,numpy.iinfo('i').max)) + '.txt'\n",
    "    #if not isfile(PREFIX + 'conlleval.pl'):\n",
    "        #os.system('wget https://www.comp.nus.edu.sg/%7Ekanmy/courses/practicalNLP_2008/packages/conlleval.pl')\n",
    "        #download('http://www-etud.iro.umontreal.ca/~mesnilgr/atis/conlleval.pl') \n",
    "        #chmod('conlleval.pl', stat.S_IRWXU) # give the execute permissions\n",
    "    #if len(PREFIX) > 0:\n",
    "        #chmod(PREFIX + 'conlleval.pl', stat.S_IRWXU) # give the execute permissions\n",
    "        #cmd = PREFIX + 'conlleval.pl < %s | grep accuracy > %s'%(filename,tempfile)\n",
    "    #else:\n",
    "        #cmd = './conlleval.pl < %s | grep accuracy > %s'%(filename,tempfile)\n",
    "    \n",
    "    cmd = './conlleval.pl < %s | grep accuracy > %s'%(filename,tempfile)\n",
    "\n",
    "    print(cmd)\n",
    "    out = os.system(cmd)\n",
    "    out = open(tempfile).readlines()[0].split()\n",
    "    os.system('rm %s'%tempfile)\n",
    "    precision = float(out[6][:-2])\n",
    "    recall    = float(out[8][:-2])\n",
    "    f1score   = float(out[10])\n",
    "    return {'p':precision, 'r':recall, 'f1':f1score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 91.79, Recall = 92.64, F1 = 92.21\n"
     ]
    }
   ],
   "source": [
    "labels_pred_val = []\n",
    "\n",
    "for n_batch, sent in enumerate(val_x):\n",
    "    label = val_label[n_batch]\n",
    "    label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "    sent = sent[np.newaxis,:]\n",
    "\n",
    "    pred = model.predict_on_batch(sent)\n",
    "    pred = np.argmax(pred,-1)[0]\n",
    "    labels_pred_val.append(pred)\n",
    "\n",
    "labels_pred_val = [ list(map(lambda x: idx2la[x], y)) \\\n",
    "                                    for y in labels_pred_val]\n",
    "con_dict = conlleval(labels_pred_val, labels_val, \n",
    "                            words_val, 'measure.txt')\n",
    "\n",
    "print('Precision = {}, Recall = {}, F1 = {}'.format(\n",
    "            con_dict['r'], con_dict['p'], con_dict['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
