{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmangia/anaconda2/envs/SwiftNLC/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Bidirectional, LSTM, GRU, TimeDistributed, Activation, Flatten, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Glove File\n",
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  # map from a token (word) to a Glove embedding vector\n",
    "        wordToIndex = {}  # map from a token to an index\n",
    "        indexToWord = {}  # map from an index to a token \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] # take the token (word) from the text line\n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) # associate the Glove embedding vector to a that token (word)\n",
    "\n",
    "        tokens = sorted(wordToGlove.keys())\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1  # 0 is reserved for masking in Keras (see above)\n",
    "            wordToIndex[tok] = kerasIdx # associate an index to a token (word)\n",
    "            indexToWord[kerasIdx] = tok # associate a word to a token (word). Note: inverse of dictionary above\n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pretrained Keras Embedding Weights Matrix\n",
    "def createPretrainedEmbeddingMatrix(wordToGlove, wordToIndex):\n",
    "    vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking\n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  # works with any glove dimensions (e.g. 50)\n",
    "\n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  # initialize with zeros\n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding\n",
    "\n",
    "    return vocabLen, embDim, embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "def getEncodedDocs(docs):\n",
    "    encoded_docs = []\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for doc in docs:\n",
    "        encoded_doc = []\n",
    "        for word in tokenizer.tokenize(doc.lower()):\n",
    "            index = wordToIndex[word]\n",
    "            if index is not None:\n",
    "                encoded_doc.append(index)\n",
    "            else:\n",
    "                encoded_doc.append(0)\n",
    "        encoded_docs.append(encoded_doc)\n",
    "\n",
    "    return encoded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GloVe Pretrained dataset and create Embedding Weight Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "wordToIndex, indexToWord, wordToGlove = readGloveFile(\"glove/glove.6B.100d.txt\")\n",
    "vocabLen, embDim, embeddingMatrix = createPretrainedEmbeddingMatrix(wordToGlove, wordToIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import intents file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('PharmacyDataset.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding, Encoding and Preparing final X and Y data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'greeting', u'goodbye', u'thanks', u'options', u'adverse_drug', u'blood_pressure', u'blood_pressure_search', u'pharmacy_search']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "encodedUtterances = []\n",
    "\n",
    "# loop through each sentence in our intents utterances\n",
    "for intent in intents['intents']:\n",
    "    classes.append(intent['intent'])\n",
    "    encoded_docs = getEncodedDocs(intent['utterances'])\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    encodedUtterances.append(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Classes / Intents array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Classes.json', 'w') as fp:\n",
    "    json.dump(classes, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'greeting', u'goodbye', u'thanks', u'options', u'adverse_drug', u'blood_pressure', u'blood_pressure_search', u'pharmacy_search'], 'classes')\n",
      "(20, 'vector size')\n",
      "(26, 20, 'x')\n",
      "(26, 8, 'y')\n",
      "(26, 20)\n",
      "(26, 8)\n"
     ]
    }
   ],
   "source": [
    "currentClass = 0\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for intent in classes:\n",
    "    y = [0] * len(classes)\n",
    "    y[currentClass] = 1\n",
    "\n",
    "    for vector in encodedUtterances[currentClass]:\n",
    "        train_x.append(vector)\n",
    "        train_y.append(y)\n",
    "\n",
    "    currentClass += 1\n",
    "\n",
    "vectorSize = len(train_x[0])\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y)\n",
    "\n",
    "print(classes, \"classes\")\n",
    "print(vectorSize, \"vector size\")\n",
    "print(len(train_x), len(train_x[0]), \"x\")\n",
    "print(len(train_y), len(train_y[0]), \"y\")\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and Compile Keras / TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Monodirectional GRU\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 100)           40000100  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20, 100)           60300     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 64)            6464      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 32)            2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 5128      \n",
      "=================================================================\n",
      "Total params: 40,074,072\n",
      "Trainable params: 73,972\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#sequence_input = Input(shape=(None, len(train_x[0])), dtype='float')\n",
    "#bidiGru = Bidirectional(GRU(100))(sequence_input)\n",
    "#preds = Dense(len(train_y[0]), activation='softmax')(bidiGru)\n",
    "#model = Model(sequence_input, preds)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "\n",
    "# NB using LSTM as CoreML only support LSTM in Bidirectional layer\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "                    Embedding(vocabLen, embDim, weights=[embeddingMatrix], input_length=max_length, trainable=False),\n",
    "                    GRU(embDim, batch_size=1, input_shape=(None, embDim), return_sequences=True),\n",
    "                    #LSTM(embDim, batch_size=1, input_shape=(None, embDim), return_sequences=True),\n",
    "                    TimeDistributed(Dense(64)),\n",
    "                    Activation('relu'),\n",
    "                    TimeDistributed(Dense(32)),\n",
    "                    Activation('relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(len(train_y[0]), activation='softmax')\n",
    "                   ])\n",
    "\n",
    "print(\"model fitting - Monodirectional GRU\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 2.0712 - acc: 0.2308\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 683us/step - loss: 2.0189 - acc: 0.3077\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 659us/step - loss: 1.9732 - acc: 0.5000\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 653us/step - loss: 1.9324 - acc: 0.5769\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 632us/step - loss: 1.8915 - acc: 0.6923\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 620us/step - loss: 1.8489 - acc: 0.7308\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 633us/step - loss: 1.8052 - acc: 0.7308\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 630us/step - loss: 1.7580 - acc: 0.6923\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 610us/step - loss: 1.7084 - acc: 0.7308\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 705us/step - loss: 1.6559 - acc: 0.7308\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 645us/step - loss: 1.5999 - acc: 0.7308\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 613us/step - loss: 1.5404 - acc: 0.7308\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 685us/step - loss: 1.4774 - acc: 0.7308\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 647us/step - loss: 1.4109 - acc: 0.7308\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 656us/step - loss: 1.3416 - acc: 0.7692\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 653us/step - loss: 1.2697 - acc: 0.7692\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 677us/step - loss: 1.1957 - acc: 0.7692\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 697us/step - loss: 1.1203 - acc: 0.7692\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 683us/step - loss: 1.0449 - acc: 0.7692\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 649us/step - loss: 0.9704 - acc: 0.7692\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 658us/step - loss: 0.8967 - acc: 0.7692\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 669us/step - loss: 0.8266 - acc: 0.7692\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 666us/step - loss: 0.7598 - acc: 0.7692\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 0s 688us/step - loss: 0.6968 - acc: 0.7692\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 645us/step - loss: 0.6381 - acc: 0.8077\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 645us/step - loss: 0.5837 - acc: 0.8077\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 647us/step - loss: 0.5330 - acc: 0.8846\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 632us/step - loss: 0.4857 - acc: 0.8846\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 644us/step - loss: 0.4414 - acc: 0.8846\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 620us/step - loss: 0.4003 - acc: 0.8846\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 648us/step - loss: 0.3623 - acc: 0.9615\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 649us/step - loss: 0.3274 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 659us/step - loss: 0.2948 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 664us/step - loss: 0.2647 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 676us/step - loss: 0.2367 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 663us/step - loss: 0.2106 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 668us/step - loss: 0.1862 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 661us/step - loss: 0.1635 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 685us/step - loss: 0.1419 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 670us/step - loss: 0.1216 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 687us/step - loss: 0.1032 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 673us/step - loss: 0.0870 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 686us/step - loss: 0.0732 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 683us/step - loss: 0.0615 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 656us/step - loss: 0.0516 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 704us/step - loss: 0.0430 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 676us/step - loss: 0.0356 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 696us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 664us/step - loss: 0.0247 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 692us/step - loss: 0.0206 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12857ffd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SwiftNLCGloveRNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.710316 \n",
      "Accuracy: 100.000000 \n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(train_X, train_Y, verbose=0)\n",
    "print('Loss: %f ' % (loss*100))\n",
    "print('Accuracy: %f ' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predict Function\n",
    "def predict(sentence):\n",
    "    test_docs = getEncodedDocs([sentence])\n",
    "    padded_test_docs = pad_sequences(test_docs, maxlen=max_length, padding='post')\n",
    "    #print(padded_test_docs)\n",
    "\n",
    "    y_pred = model.predict(padded_test_docs)\n",
    "    #print(y_pred)\n",
    "\n",
    "    max_value = max(y_pred[0])\n",
    "    #print(max_value)\n",
    "\n",
    "    max_index = y_pred[0].tolist().index(max_value)\n",
    "    #print(max_index)\n",
    "\n",
    "    #print(intents['intents'][max_index]['intent'])\n",
    "    return intents['intents'][max_index]['intent'], max_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\n",
      "(u'Hi', u'greeting', 0.99859434)\n",
      "(u'How are you', u'greeting', 0.9955049)\n",
      "(u'Is anyone there?', u'greeting', 0.99886274)\n",
      "(u'Hello', u'greeting', 0.9758414)\n",
      "(u'Good day', u'greeting', 0.99947256)\n",
      "goodbye\n",
      "(u'Bye', u'goodbye', 0.9629647)\n",
      "(u'See you later', u'goodbye', 0.94820195)\n",
      "(u'Goodbye', u'goodbye', 0.9159323)\n",
      "thanks\n",
      "(u'Thanks', u'thanks', 0.8995135)\n",
      "(u'Thank you', u'thanks', 0.9359834)\n",
      "(u\"That's helpful\", u'thanks', 0.9906031)\n",
      "options\n",
      "(u'How you could help me?', u'options', 0.99877554)\n",
      "(u'What you can do?', u'options', 0.9839819)\n",
      "(u'What help you provide?', u'options', 0.9979873)\n",
      "adverse_drug\n",
      "(u'How to check Adverse drug reaction?', u'adverse_drug', 0.99986017)\n",
      "(u'List all drugs suitable for patient with adverse reaction', u'adverse_drug', 0.9966732)\n",
      "(u'Which drugs dont have adverse reaction?', u'adverse_drug', 0.9970788)\n",
      "blood_pressure\n",
      "(u'Open blood pressure module', u'blood_pressure', 0.9871093)\n",
      "(u'I want to log blood pressure results', u'blood_pressure', 0.9984921)\n",
      "(u'Blood pressure data management', u'blood_pressure', 0.9976127)\n",
      "blood_pressure_search\n",
      "(u'I want to search for blood pressure result history', u'blood_pressure_search', 0.9999275)\n",
      "(u'Show blood pressure results for patient', u'blood_pressure_search', 0.9999565)\n",
      "(u'Find blood pressure results by ID', u'blood_pressure_search', 0.9997422)\n",
      "pharmacy_search\n",
      "(u'Find me a pharmacy', u'pharmacy_search', 0.99111825)\n",
      "(u'Locate pharmacy by name', u'pharmacy_search', 0.9998908)\n",
      "(u'Search pharmacy by parameter', u'pharmacy_search', 0.999653)\n"
     ]
    }
   ],
   "source": [
    "# Cheating TEST\n",
    "for intentGroup in intents[\"intents\"]:\n",
    "    print(intentGroup[\"intent\"])\n",
    "    for phrase in intentGroup[\"utterances\"]:\n",
    "        intent, prob = predict(phrase)\n",
    "        print(phrase, intent, prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Display blood values for patient', u'blood_pressure_search', 0.97529125)\n",
      "('That was very helpful', u'greeting', 0.55592704)\n",
      "('Show me the patient blood pressure results', u'blood_pressure_search', 0.7962213)\n",
      "('Search for a pharmacy please', u'pharmacy_search', 0.9906324)\n",
      "('How can you help me', u'options', 0.99933475)\n"
     ]
    }
   ],
   "source": [
    "#Test Dataset TEST\n",
    "\n",
    "testDataset = [ \"Display blood values for patient\",\n",
    "                \"That was very helpful\",\n",
    "                \"Show me the patient blood pressure results\",\n",
    "                \"Search for a pharmacy please\",\n",
    "                \"How can you help me\" ]\n",
    "\n",
    "\n",
    "\n",
    "for phrase in testDataset:\n",
    "    intent, prob = predict(phrase)\n",
    "    print(phrase, intent, prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Word Embedding Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('biennials', 75684, 0)\n",
      "('tripolitan', 365444, 1)\n",
      "('biysk', 77319, 2)\n",
      "('woode', 389559, 3)\n",
      "('verplank', 377800, 4)\n",
      "('mdbo', 239051, 5)\n",
      "('sowell', 338477, 6)\n",
      "('mdbu', 239054, 7)\n",
      "('soestdijk', 336526, 8)\n",
      "('spiders', 339422, 9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(wordToIndex.keys()[i], wordToIndex[wordToIndex.keys()[i]], i)\n",
    "    \n",
    "import json\n",
    "with open('Words.json', 'w') as fp:\n",
    "    json.dump(wordToIndex, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model using CoreML Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Keras version 2.1.5 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n",
      "WARNING:root:TensorFlow version 1.6.0 detected. Last version known to be fully compatible is 1.5.0 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : embedding_1_input, <keras.engine.topology.InputLayer object at 0x147b5b590>\n",
      "1 : embedding_1, <keras.layers.embeddings.Embedding object at 0x11b898c90>\n",
      "2 : gru_1, <keras.layers.recurrent.GRU object at 0x11ac01050>\n",
      "3 : time_distributed_1, <keras.layers.wrappers.TimeDistributed object at 0x147b5b190>\n",
      "4 : activation_1, <keras.layers.core.Activation object at 0x147b5b1d0>\n",
      "5 : time_distributed_2, <keras.layers.wrappers.TimeDistributed object at 0x147b5b310>\n",
      "6 : activation_2, <keras.layers.core.Activation object at 0x147b5b350>\n",
      "7 : flatten_1, <keras.layers.core.Flatten object at 0x147b5b390>\n",
      "8 : dense_3, <keras.layers.core.Dense object at 0x147b5b410>\n",
      "9 : dense_3__activation__, <keras.layers.core.Activation object at 0x12a324a90>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input {\n",
       "  name: \"vectors\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 1\n",
       "      dataType: DOUBLE\n",
       "    }\n",
       "  }\n",
       "}\n",
       "input {\n",
       "  name: \"gru_1_h_in\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 100\n",
       "      dataType: DOUBLE\n",
       "    }\n",
       "    isOptional: true\n",
       "  }\n",
       "}\n",
       "output {\n",
       "  name: \"entities\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 8\n",
       "      dataType: DOUBLE\n",
       "    }\n",
       "  }\n",
       "}\n",
       "output {\n",
       "  name: \"gru_1_h_out\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 100\n",
       "      dataType: DOUBLE\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import coremltools\n",
    "coreml_model = coremltools.converters.keras.convert(model, input_names=\"vectors\", output_names=\"entities\")\n",
    "coreml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.save('SwiftNLCGloveRNN.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
